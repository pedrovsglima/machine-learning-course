{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eVey53yXnDE"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "## General information\n",
    "\n",
    "This assignment has three problems:\n",
    " - 1: Generative Classifiers\n",
    " - 2: Softmax Regression\n",
    " - 3: Ensemble Classifiers\n",
    "\n",
    "Utilize the designated cells within this notebook to complete the exercises. As for the Python exercises:\n",
    "- Refrain from altering the provided code; simply fill in the missing portions as indicated.\n",
    "- Do not use any additional libraries beyond those already included in the code (e.g., NumPy library).\n",
    "- Make sure that the output of all code cells is visible in your submitted notebook. **The evaluator is NOT expected to execute your code before grading your submission.**\n",
    "- Some problems include automatic checks (*assertions*) to verify basic aspects of your solution. However, passing these assertions does not guarantee that your solution is entirely correct. The final evaluation will always be determined by the evaluator.\n",
    "- Avoid the use of for loops by using NumPy vectorized operations whenever possible.\n",
    "   \n",
    "Please identify the authors of this assignment in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM05fvVqQA-R"
   },
   "source": [
    "## Identification\n",
    "\n",
    "* **Name:**\n",
    "* **Student Number:**\n",
    "\n",
    "* **Name:**\n",
    "* **Student Number:**\n",
    "\n",
    "* **Name:**\n",
    "* **Student Number:**\n",
    "---\n",
    "\n",
    "**Note:** This work is to be done in group of up to **3** elements. Use this notebook to answer all the questions. At the end of the work, you should **upload** the **notebook** and a **pdf file** with a printout of the notebook with all the results in the **moodle** platform.\n",
    "To generate the pdf file we have first to covert the notebook to html using the command `!jupyter nbconvert --to html \"ML_project2.ipynb\"`, then open the html file and printout to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFvjQVHQBLt"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "For this assignment, we will consider the task of predicting the quality of red wine (rated on an integer scale from 0 to 5) based on 11 chemical properties, such as pH, alcohol content, residual sugar, etc. This dataset was originally published in:\n",
    "\n",
    "Cortez, Paulo, et al. [\"Modeling wine preferences by data mining from physicochemical properties.\"](https://https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377) Decision support systems 47.4 (2009): 547-553.\n",
    "\n",
    "Since quality is a discrete and ordinal attribute, this problem could be framed as either a regression or classification task. For this assignment, we will treat it as a classification problem. Specifically, we have a **6-class classification dataset** ($y \\in \\{0, 1, \\dots, 5\\}$) where each **feature vector is 11-dimensional** ($\\mathbf{x} \\in \\mathbb{R}^{11}$).\n",
    "\n",
    "In the cell below, we load and prepare the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "lvDtT4_8MkMX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A training example:\n",
      "fixed acidity            8.60000\n",
      "volatile acidity         0.22000\n",
      "citric acid              0.36000\n",
      "residual sugar           1.90000\n",
      "chlorides                0.06400\n",
      "free sulfur dioxide     53.00000\n",
      "total sulfur dioxide    77.00000\n",
      "density                  0.99604\n",
      "pH                       3.47000\n",
      "sulphates                0.87000\n",
      "alcohol                 11.00000\n",
      "quality                  4.00000\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "A test example:\n",
      "fixed acidity            7.7000\n",
      "volatile acidity         0.5600\n",
      "citric acid              0.0800\n",
      "residual sugar           2.5000\n",
      "chlorides                0.1140\n",
      "free sulfur dioxide     14.0000\n",
      "total sulfur dioxide    46.0000\n",
      "density                  0.9971\n",
      "pH                       3.2400\n",
      "sulphates                0.6600\n",
      "alcohol                  9.6000\n",
      "quality                  3.0000\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "X_train shape: (1119, 11)\n",
      "X_test shape: (480, 11)\n",
      "y_train shape: (1119,)\n",
      "y_test shape: (480,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"winequality_train.csv\")\n",
    "print(\"A training example:\")\n",
    "print(train_data.iloc[0])\n",
    "print()\n",
    "test_data = pd.read_csv(\"winequality_test.csv\")\n",
    "print(\"A test example:\")\n",
    "print(test_data.iloc[0])\n",
    "print()\n",
    "\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = train_data.iloc[:, -1].values\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl7wtbg5AnHk"
   },
   "source": [
    "An important **preprocessing step** for multi-dimensional continuous data is to ensure all features have **zero mean** and **unit variance**. In the code below, we transform the data to achieve this.\n",
    "\n",
    "**When normalizing the test data, we reuse the mean and standard deviation from the training data**, rather than recalculating them. This ensures a fair evaluation, since we want to classify new examples without assuming any prior knowledge about their distribution beyond what was learned during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "xnmjozR85Jpj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data statistics before normalization:\n",
      "  * mean: [ 8.30956211  0.53313226  0.27025022  2.54830206  0.08771135 15.9204647\n",
      " 46.96648794  0.996778    3.31427167  0.65882038 10.41733691]\n",
      "  * std: [1.71313271e+00 1.81940308e-01 1.95404143e-01 1.42709225e+00\n",
      " 4.71218617e-02 1.02685749e+01 3.30219278e+01 1.83946303e-03\n",
      " 1.53911124e-01 1.72165005e-01 1.05927763e+00]\n",
      "\n",
      "Test data statistics before normalization:\n",
      "  * mean: [ 8.343125    0.5154375   0.27266667  2.51666667  0.08689583 15.76875\n",
      " 45.30520833  0.99667367  3.30375     0.65658333 10.43614583]\n",
      "  * std: [1.80263540e+00 1.71324851e-01 1.93172994e-01 1.36730930e+00\n",
      " 4.68790285e-02 1.08825004e+01 3.25340058e+01 1.99070295e-03\n",
      " 1.55079617e-01 1.62948079e-01 1.07920724e+00]\n",
      "\n",
      "Training data statistics after normalization:\n",
      "  * mean: [ 5.42907988e-16 -2.34148913e-16  1.65094827e-16 -6.66729109e-17\n",
      "  3.41301806e-16  7.93725129e-18  1.58745026e-17 -4.02418641e-14\n",
      " -8.25474134e-16 -4.42898622e-16  1.17471319e-16]\n",
      "  * std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Test data statistics after normalization:\n",
      "  * mean: [ 0.01959153 -0.09725586  0.01236639 -0.02216773 -0.01730653 -0.01477466\n",
      " -0.05030838 -0.05671848 -0.06836199 -0.01299359  0.01775637]\n",
      "  * std: [1.05224505 0.94165418 0.98858187 0.95810856 0.9948467  1.05978683\n",
      " 0.9852243  1.0822196  1.007592   0.94646458 1.01881434]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data statistics before normalization:\")\n",
    "print(\"  * mean:\", X_train.mean(axis=0))\n",
    "print(\"  * std:\", X_train.std(axis=0))\n",
    "print()\n",
    "print(\"Test data statistics before normalization:\")\n",
    "print(\"  * mean:\", X_test.mean(axis=0))\n",
    "print(\"  * std:\", X_test.std(axis=0))\n",
    "print()\n",
    "\n",
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test = (X_test - train_mean) / train_std\n",
    "\n",
    "print(\"Training data statistics after normalization:\")\n",
    "print(\"  * mean:\", X_train.mean(axis=0))\n",
    "print(\"  * std:\", X_train.std(axis=0))\n",
    "print()\n",
    "print(\"Test data statistics after normalization:\")\n",
    "print(\"  * mean:\", X_test.mean(axis=0))\n",
    "print(\"  * std:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szaZLcuDZQp7"
   },
   "source": [
    "## Problem 1: Generative Classifiers (Linear Discriminant Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpCEelC1Z9X5"
   },
   "source": [
    "Consider a classifier that predicts the class label $\\widehat{y}$ for an observed feature vector $\\mathbf{x}$ using the *maximum a posteriori* (MAP) classification rule:\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\arg\\max_y p(y \\mid \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Assume the following:\n",
    "1. The distribution of $\\mathbf{x} \\mid y\\$ is Gaussian.\n",
    "2. The covariance matrix is shared by all the classes.\n",
    "\n",
    "In the cell below, you have the skeleton of a `LDAClassifier` class that you will complete throughout this exercise to implement this classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "7gUX6cTxY2CS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class LDAClassifier:\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Estimate the parameters of the classifier from the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "        y: np.ndarray of shape (num_examples,)\n",
    "            The corresponding labels (target values).\n",
    "        \"\"\"\n",
    "        # class priors\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        self.prior_hat = counts / len(y)\n",
    "        if not np.isclose(np.sum(self.prior_hat), 1, atol=1e-8):\n",
    "            raise ValueError(\"The sum of the prior values must be 1 (or close enough).\")\n",
    "        \n",
    "        # mean vectors\n",
    "        self.mean_hat = np.array([X[y==k].mean(axis=0) for k in self.classes])\n",
    "        \n",
    "        # shared covariance matrix\n",
    "        diff = np.vstack([X[y == k] - self.mean_hat[k] for k in self.classes])\n",
    "        self.cov_matrix_hat = (diff.T @ diff) / len(X)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_examples, num_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        det_covariance = np.linalg.det(self.cov_matrix_hat)\n",
    "        if det_covariance == 0:\n",
    "            raise ValueError(\"Covariance matrix is not invertible.\")\n",
    "        \n",
    "        likelihoods = np.array([\n",
    "            multivariate_normal.pdf(X, mean=self.mean_hat[k], cov=self.cov_matrix_hat)\n",
    "            for k in self.classes\n",
    "        ]).T\n",
    "        \n",
    "        probs = likelihoods * self.prior_hat\n",
    "        probs /= probs.sum(axis=1, keepdims=True) # normalization\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data using the estimated model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features) for which predictions are needed.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_examples,)\n",
    "            The predicted class labels.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVHMijtxYpoE"
   },
   "source": [
    "### 1.1\n",
    "Based on the assumptions given before, **identify all the parameters** in the model and **write down the expressions for their maximum likelihood estimates**. You do not need to show the derivation of the expressions, just the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiJf1aP7cZkT"
   },
   "source": [
    "*Your answer here*\n",
    "\n",
    "Considering $K$ as the number of classes (for the Wine Quality dataset $K=6$) and $N$ as the total number of samples, the parameters are:\n",
    "\n",
    "1. Class priors: \n",
    "\n",
    "- $p(y=k) = \\pi_k, \\text{ for } k = 0, ..., K-1$\n",
    "\n",
    "- Let $n_k$ be the number of samples in class $k$:\n",
    "\n",
    "$$\\hat\\pi_k = \\frac{n_k}{N}$$\n",
    "\n",
    "Note: $\\sum_{k=0}^{K-1} \\hat\\pi_k = 1$\n",
    "\n",
    "2. Mean vectors:\n",
    "\n",
    "- $\\mu_k, \\text{ for } k = 0, ..., K-1$\n",
    "\n",
    "- Let $C_k$ be the set of indices for samples in class $k$ and $x_i$ the feature vector for sample $i$:\n",
    "\n",
    "$$\\hat\\mu_k = \\frac{1}{n_k} \\sum_{i\\in C_k} x_i$$\n",
    "\n",
    "3. Shared covariance matrix $\\Sigma$ :\n",
    "\n",
    "Let $d$ be the dimensionality of the data (for the Wine Quality dataset $d=11$):\n",
    "\n",
    "$$\n",
    "\\hat\\Sigma = \n",
    "\\begin{bmatrix}\n",
    "\\hat\\Sigma_{1,1} & \\hat\\Sigma_{1,2} & \\cdots & \\hat\\Sigma_{1,d} \\\\\n",
    "\\hat\\Sigma_{2,1} & \\hat\\Sigma_{2,2} & \\cdots & \\hat\\Sigma_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\hat\\Sigma_{d,1} & \\hat\\Sigma_{d,2} & \\cdots & \\hat\\Sigma_{d,d}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "each element $\\hat\\Sigma_{p,q}$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat\\Sigma_{p,q} = \\frac{1}{N} \\sum_{k=0}^{K-1} \\sum_{i \\in C_k} \\big( x_{i,p} - \\hat{\\mu}_{k,p} \\big) \\big( x_{i,q} - \\hat{\\mu}_{k,q} \\big),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $p,q \\in \\{1,â€¦,d\\}$\n",
    "- $x_{i,p}$ is the $p$-th component of the $i$-th sample $x_i$\n",
    "- $x_{i,q}$ is the $q$-th component of the $i$-th sample $x_i$\n",
    "- $\\hat\\mu_{k,p}$ is the $p$-th component of the mean vector $\\hat\\mu_k$ for class $k$\n",
    "- $\\hat\\mu_{k,q}$ is the $q$-th component of the mean vector $\\hat\\mu_k$ for class $k$\n",
    "- $C_k$ is the set of indices for samples in class k\n",
    "\n",
    "\n",
    "Finally, knowing that\n",
    "\n",
    "$$\n",
    "p(y = k \\mid \\mathbf{x}) \\propto p(\\mathbf{x} \\mid y = k) p(y = k),\n",
    "$$\n",
    "\n",
    "and the likelihood $p(\\mathbf{x} \\mid y = k)$ for a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid y = k) = \\frac{1}{(2\\pi)^{d/2} \\lvert \\hat{\\Sigma}_k \\rvert^{1/2}} \\exp \\left[ -\\frac{1}{2} (\\mathbf{x} - \\hat{\\mu}_k)^\\top \\hat{\\Sigma}_k^{-1} (\\mathbf{x} - \\hat{\\mu}_k) \\right].\n",
    "$$\n",
    "\n",
    "We apply $\\log$ for the sake of computational simplicity\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\arg\\max_k \\log p(y = k \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "and the decision rule is then derived by considering the $\\log$ of the likelihood and the class prior:\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\arg\\max_k \\left[ \\log p(y = k) - \\frac{1}{2} (\\mathbf{x} - \\hat{\\mu}_k)^\\top \\hat{\\Sigma}^{-1} (\\mathbf{x} - \\hat{\\mu}_k) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\arg\\max_{k} \\left[ \\log \\hat{\\pi}_k - \\frac{1}{2} (\\mathbf{x} - \\hat{\\mu}_k)^\\top \\hat{\\Sigma}^{-1} (\\mathbf{x} - \\hat{\\mu}_k) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf85q0JTdhpM"
   },
   "source": [
    "### 1.2\n",
    "In this exercise, you should write the code for the `fit` method of this classifier. This method should **estimate all model parameters** using the data provided as arguments. Specifically:\n",
    "\n",
    "- `X`: A NumPy array of shape `(num_examples, num_features)` containing the training input examples $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n$.\n",
    "- `y`: A NumPy array of shape `(num_examples,)` containing the corresponding labels $y_1, y_2, \\dots, y_n$.\n",
    "\n",
    "The model parameters should be saved as class attributes, so they can be used later during prediction.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass and that the model parameter names and values are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "OXq8kE-PZgzF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier attributes:\n",
      "  * classes, shape (6,): [0 1 2 3 4 5]\n",
      "  * prior_hat, shape (6,): [0.0080429  0.03217158 0.43431635 0.39142091 0.1233244  0.01072386]\n",
      "  * mean_hat, shape (6, 11): [[ 1.37106912e-01  2.00725764e+00 -4.22060885e-01  1.25763224e-01\n",
      "   8.43198752e-01 -4.25074483e-01 -6.24831922e-01  4.61367497e-01\n",
      "   4.34272965e-01 -5.54625151e-01 -4.35939868e-01]\n",
      " [-2.69879785e-01  6.80509915e-01 -3.01228692e-01  5.76371290e-02\n",
      "   1.74129734e-01 -2.46536238e-01 -2.00871210e-01 -1.57623531e-01\n",
      "   2.66427180e-01 -2.57752328e-01 -2.01241142e-01]\n",
      " [-6.28732712e-02  2.80444275e-01 -1.28167508e-01  1.39499189e-02\n",
      "   8.84356273e-02  1.03727345e-01  3.17583390e-01  2.08867168e-01\n",
      "  -6.18446834e-02 -2.18073674e-01 -5.01012554e-01]\n",
      " [ 8.15345524e-04 -1.75140616e-01  5.84670637e-03 -6.61630507e-02\n",
      "  -4.65407032e-02 -2.28261122e-02 -1.98174787e-01 -8.46028330e-02\n",
      "   7.80116953e-02  1.06442906e-01  2.14973258e-01]\n",
      " [ 2.43136358e-01 -6.90115339e-01  4.72653602e-01  1.26863397e-01\n",
      "  -2.32386401e-01 -1.77496745e-01 -3.57332901e-01 -4.13927324e-01\n",
      "  -8.47228373e-02  4.78677468e-01  1.04271341e+00]\n",
      " [ 4.27348421e-01 -5.76007202e-01  7.62094604e-01  1.23816764e-01\n",
      "  -3.65251898e-01 -2.68177237e-01 -4.48181625e-01 -4.84107698e-01\n",
      "  -4.93390835e-01  6.31252702e-01  1.38395863e+00]]\n",
      "  * cov_matrix_hat, shape (11, 11): [[ 0.98653963 -0.21102171  0.63732857  0.11843414  0.11064244 -0.13762382\n",
      "  -0.08292385  0.67511944 -0.67329096  0.1386818  -0.09713923]\n",
      " [-0.21102171  0.84423858 -0.47674432 -0.00500469  0.01753982 -0.03547893\n",
      "   0.00118994 -0.0552199   0.21131167 -0.17677932 -0.02521314]\n",
      " [ 0.63732857 -0.47674432  0.95472107  0.16698051  0.24093041 -0.04236807\n",
      "   0.08841737  0.40842579 -0.52382397  0.25481359  0.01041678]\n",
      " [ 0.11843414 -0.00500469  0.16698051  0.99581871  0.06440351  0.15181822\n",
      "   0.16566239  0.37809667 -0.08987927  0.00874431  0.04286123]\n",
      " [ 0.11064244  0.01753982  0.24093041  0.06440351  0.98097099 -0.00915295\n",
      "   0.01433735  0.16530113 -0.26599429  0.39228789 -0.13742129]\n",
      " [-0.13762382 -0.03547893 -0.04236807  0.15181822 -0.00915295  0.98705784\n",
      "   0.63969949 -0.04760898  0.05795171  0.06905586 -0.04366945]\n",
      " [-0.08292385  0.00118994  0.08841737  0.16566239  0.01433735  0.63969949\n",
      "   0.9184837   0.01638628 -0.06294274  0.09037041 -0.09583213]\n",
      " [ 0.67511944 -0.0552199   0.40842579  0.37809667  0.16530113 -0.04760898\n",
      "   0.01638628  0.95209663 -0.32803867  0.18123705 -0.35938944]\n",
      " [-0.67329096  0.21131167 -0.52382397 -0.08987927 -0.26599429  0.05795171\n",
      "  -0.06294274 -0.32803867  0.98866046 -0.18799918  0.19889069]\n",
      " [ 0.1386818  -0.17677932  0.25481359  0.00874431  0.39228789  0.06905586\n",
      "   0.09037041  0.18123705 -0.18799918  0.93776851 -0.01796635]\n",
      " [-0.09713923 -0.02521314  0.01041678  0.04286123 -0.13742129 -0.04366945\n",
      "  -0.09583213 -0.35938944  0.19889069 -0.01796635  0.71543592]]\n"
     ]
    }
   ],
   "source": [
    "# instantiate the classifier and train it\n",
    "lda_clf = LDAClassifier()\n",
    "lda_clf.fit(X_train, y_train)\n",
    "\n",
    "assert lda_clf.__dict__.keys(), (\"`fit` method not implemented yet or not \"\n",
    "\"creating the class attributes.\")\n",
    "print(\"Classifier attributes:\")\n",
    "for attr, value in lda_clf.__dict__.items():\n",
    "    shape = f\", shape {value.shape}\" if hasattr(value, \"shape\") else \"\"\n",
    "    print(f\"  * {attr}{shape}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSLu9JeJbMBS"
   },
   "source": [
    "### 1.3\n",
    "Implement the `predict_proba` method of the `LDAClassifier` class. This method should use the model parameters estimated during the `fit` method to **predict the probabilities of each class** for new input data. Specifically, the input is:\n",
    "\n",
    "- `X`: A NumPy array of shape `(num_examples, num_features)` containing the input examples $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n$ for which predictions are needed.\n",
    "\n",
    "The method should return a NumPy array of shape `(num_examples, num_classes)` containing the predicted probabilities $p(y=0 \\mid \\mathbf{x}_i), p(y=1 \\mid \\mathbf{x}_i), \\dots, p(y=C-1 \\mid \\mathbf{x}_i)$, for $i \\in \\{1, 2, \\dots, n\\}$.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "S-7e3TzWcOnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your `predict_proba` looks good! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# instantiate the classifier\n",
    "# (we need to do this again because you probably changed the code of LDAClassifier\n",
    "# after you ran the previous cell)\n",
    "lda_clf = LDAClassifier()\n",
    "lda_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the class probabilities for the training data\n",
    "probs = lda_clf.predict_proba(X_train)\n",
    "\n",
    "assert probs is not None, (\"`predict_proba` method not implemented yet or \"\n",
    "\"not returning a valid array.\")\n",
    "assert probs.shape == (len(X_train), 6), (\"`predict_proba` output should \"\n",
    "\"have shape (num_examples, num_classes).\")\n",
    "assert np.all((probs >= 0) * (probs <= 1)), (\"`predict_proba` output should be \"\n",
    "\"probabilities.\")\n",
    "assert np.allclose(probs.sum(axis=1), 1), (\"`predict_proba` output should be a \"\n",
    "\"(proper) probability distribution.\")\n",
    "print(\"Your `predict_proba` looks good! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 6)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: excluir\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4LLeAaQgLzQ"
   },
   "source": [
    "### 1.4\n",
    "Implement the `predict` method of the `LDAClassifier` class. This method should **predict the class labels** for new input data, by applying the MAP rule. Specifically, the input is:\n",
    "\n",
    "- `X`: A NumPy array of shape `(num_examples, num_features)` containing the input examples $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n$ for which predictions are needed.\n",
    "\n",
    "The method should return a NumPy array of shape `(num_examples,)` containing the predicted class labels $\\widehat{y}_1, \\widehat{y}_2, \\dots, \\widehat{y}_n$.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass and that a summary of the model predictions is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "pp4b2ax3eoYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your classifier predicted:\n",
      "  * 9 training examples in class 0.\n",
      "  * 1 training examples in class 1.\n",
      "  * 546 training examples in class 2.\n",
      "  * 451 training examples in class 3.\n",
      "  * 111 training examples in class 4.\n",
      "  * 1 training examples in class 5.\n",
      "\n",
      "Your classifier predicted:\n",
      "  * 3 test examples in class 0.\n",
      "  * 0 test examples in class 1.\n",
      "  * 227 test examples in class 2.\n",
      "  * 191 test examples in class 3.\n",
      "  * 59 test examples in class 4.\n",
      "  * 0 test examples in class 5.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the classifier\n",
    "# (we need to do this again because you probably changed the code of LDAClassifier\n",
    "# after you ran the previous cell)\n",
    "lda_clf = LDAClassifier()\n",
    "lda_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the class labels for the training data\n",
    "y_train_pred = lda_clf.predict(X_train)\n",
    "assert y_train_pred is not None, (\"`predict` method not implemented yet or not returning \"\n",
    "\"any predictions.\")\n",
    "assert len(y_train_pred) == len(X_train), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_train_pred == j).sum()} training examples in class {j}.\")\n",
    "print()\n",
    "\n",
    "# predict the class labels for the test data\n",
    "y_test_pred = lda_clf.predict(X_test)\n",
    "assert y_test_pred is not None, (\"`predict` method not implemented yet or not returning \"\n",
    "\"any predictions.\")\n",
    "assert len(y_test_pred) == len(X_test), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_test_pred == j).sum()} test examples in class {j}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6465-BowhHoB"
   },
   "source": [
    "### 1.5\n",
    "Complete the code of the function `compute_accuracy`, which **computes the accuracy** of a given set of predictions when compared to the ground-truth labels. This function will be used to compute the training and test accuracies of our `LDAClassifier`.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass and that the values of the training and test accuracies are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "OFq5rhpzhG5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.621\n",
      "Test accuracy: 0.562\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the classifier.\n",
    "\n",
    "    Parameters:\n",
    "    y_true: np.ndarray of shape (num_examples,)\n",
    "        The true class labels.\n",
    "    y_pred: np.ndarray of shape (num_examples,)\n",
    "        The predicted class labels.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The accuracy of the classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shape mismatch. They must be identical.\")\n",
    "\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    \n",
    "    total_predictions = len(y_true)\n",
    "\n",
    "    return correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "\n",
    "# compute the training accuracy\n",
    "train_accuracy = compute_accuracy(y_train, y_train_pred)\n",
    "assert train_accuracy is not None, (\"`compute_accuracy` not implemented yet or \"\n",
    "\"not returning a valid float.\")\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "# compute the test accuracy\n",
    "test_accuracy = compute_accuracy(y_test, y_test_pred)\n",
    "assert test_accuracy is not None, (\"`compute_accuracy` not implemented yet or \"\n",
    "\"not returning a valid float.\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCBIKZOiB4_N"
   },
   "source": [
    "### 1.6\n",
    "Explain why normalizing features to have zero mean and unit variance is an important preprocessing step **for this classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nbSsfDoCSCv"
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Xz8j4Ulnbr"
   },
   "source": [
    "## Problem 2: Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WzVTofAlrl7"
   },
   "source": [
    "The **softmax regression** (also known as **multinomial logistic regression**) is a generalization of the logistic regression to multi-class (i.e., non-binary) classification problems. For a given input $\\mathbf{x}$, the raw score *(logit)* for class $y$ is calculated as:\n",
    "\n",
    "$$\n",
    "z_y = \\mathbf{w}_y^\\top \\mathbf{x} + b_y,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w}_y$ is the weight vector for class $y$,\n",
    "- $\\mathbf{x}$ is the input feature vector,\n",
    "- $b_y$ is the bias term for class $y$,\n",
    "- $z_y$ is the logit (raw score) for class $y$.\n",
    "\n",
    "The *softmax* function converts these logits into probabilities $p(y \\mid \\mathbf{x})$, for $y \\in \\{0, 1, \\dots, C-1\\}$, as follows:\n",
    "\n",
    "$$\n",
    "p(y \\mid \\mathbf{x}) = \\frac{\\exp(z_y)}{\\sum_{j=0}^{C-1} \\exp(z_j)}.\n",
    "$$\n",
    "\n",
    "As with logistic regression, the parameters of softmax regression are usually estimated by maximum likelihood, i.e. by minimizing the function:\n",
    "\n",
    "$$\\ell(\\mathbf{\\theta}) = -\\frac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid \\mathbf{x}_i; \\mathbf{\\theta}),$$\n",
    "\n",
    "where $\\mathbf{\\theta}$ denotes all model parameters ($\\mathbf{w}_0, \\dots, \\mathbf{w}_{C-1}, b_0, \\dots, b_{C-1}$) and $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\dots, (\\mathbf{x}_n, y_n)\\}$ is a given training set of labeled examples. As with logistic regression, there is no closed-form solution for the maximum likelihood estimates of these parameters, so they must be estimated using gradient-based optimization.\n",
    "\n",
    "In the cell below, you have the skeleton of a `SoftmaxRegressionClassifier` class that you will complete throughout this exercise to implement this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeFtsBU3ygp5"
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegressionClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.01,\n",
    "        max_iterations: int = 10000,\n",
    "        tolerance: float = 1e-5,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the softmax regression classifier.\n",
    "\n",
    "        Parameters:\n",
    "        learning_rate: float\n",
    "            The learning rate for gradient descent.\n",
    "        max_iterations: int\n",
    "            The maximum number of training iterations.\n",
    "        tolerance: float\n",
    "            The maximum tolerance for the loss improvement.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        # parameters will be initialized properly when we call `fit`\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the softmax probabilities for the input logits.\n",
    "\n",
    "        Parameters:\n",
    "        logits: np.ndarray of shape (num_examples, num_classes)\n",
    "            The raw class scores (logits) from the model.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_examples, num_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        # ToDo: Exercise 2.1\n",
    "        # **Replace `pass` with your code**\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class probabilities for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_examples, num_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        # ToDo: Exercise 2.2\n",
    "        # **Replace `pass` with your code**\n",
    "        pass\n",
    "\n",
    "    def compute_loss_and_gradients(self, X: np.ndarray, y: np.ndarray) -> tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute the loss and the gradients of the weights and biases for the softmax regression model.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "        y: np.ndarray of shape (num_examples,)\n",
    "            The corresponding labels (target values).\n",
    "\n",
    "        Returns:\n",
    "        loss: float\n",
    "            The loss value.\n",
    "        dweights: np.ndarray of shape (num_features, num_classes)\n",
    "            The gradient of the loss with respect to the weights.\n",
    "        dbiases: np.ndarray of shape (num_classes,)\n",
    "            The gradient of the loss with respect to the biases.\n",
    "        \"\"\"\n",
    "        # ToDo: Exercise 2.3\n",
    "        # **Replace `pass` with your code**\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Train the softmax regression model using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "        y: np.ndarray of shape (num_examples,)\n",
    "            The corresponding labels (target values).\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_training_iterations,)\n",
    "            The loss values obtained on each training iteration.\n",
    "        \"\"\"\n",
    "        num_examples, num_features = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "\n",
    "        # parameters initialization\n",
    "        self.weights = np.random.randn(num_features, num_classes)  # weights are initialized randomly\n",
    "        self.biases = np.zeros((num_classes,))  # biases are initialized with zeros\n",
    "\n",
    "        # ToDo: Exercise 2.4\n",
    "        # **Replace `pass` with your code**\n",
    "        pass\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        X: np.ndarray of shape (num_examples, num_features)\n",
    "            The input data (features).\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray of shape (num_examples,)\n",
    "            The predicted class labels.\n",
    "        \"\"\"\n",
    "        # ToDo: Exercise 2.5\n",
    "        # **Replace `pass` with your code**\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvqLeAyspvKj"
   },
   "source": [
    "### 2.1\n",
    "Now we'll start implementing our softmax regression classifier by filling the skeleton of the `SoftmaxRegressionClassifier` class defined above.\n",
    "\n",
    "For this exercise, you should only **implement the `softmax` method**, which takes as input the `logits` array with shape `(num_examples, num_classes)` and returns the output probabilities for each class in an array with the same shape.\n",
    "\n",
    "The softmax function involves exponentials, which can lead to large numbers and potential numerical instability. To address this, we will use a common trick: subtract the maximum logit value from all logits before applying the exponential function:\n",
    "\n",
    "$$\n",
    "p(y \\mid \\mathbf{x}) = \\frac{\\exp(z_y)}{\\sum_{j=0}^{C-1} \\exp(z_j)} = \\frac{\\exp(z_y - z_{\\max})}{\\sum_{j=0}^{C-1} \\exp(z_j-z_{\\max})},\n",
    "$$\n",
    "\n",
    "where $z_{\\max} = \\max(z_0, z_1, \\dots, z_{C-1})$. This transformation does not affect the final result but ensures the values remain within a stable range. In your solution, **you should implement this trick** to ensure numerical stability.\n",
    "\n",
    "Run the cell below after you implement the function and make sure all the assertions pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3VR3wD16Uve"
   },
   "outputs": [],
   "source": [
    "logits = np.random.randn(10, 6)\n",
    "probs = SoftmaxRegressionClassifier.softmax(logits)\n",
    "\n",
    "assert probs is not None, (\"`softmax` method not implemented yet or not \"\n",
    "\"returning a valid array.\")\n",
    "assert probs.shape == logits.shape, (\"`softmax` output should have the same \"\n",
    "\"shape as the input.\")\n",
    "assert np.all((probs >= 0) * (probs <= 1)), (\"`softmax` output should be \"\n",
    "\"probabilities.\")\n",
    "assert np.allclose(probs.sum(axis=1), 1), (\"`softmax` output should be a \"\n",
    "\"(proper) probability distribution.\")\n",
    "print(\"Your `softmax` looks good! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZfENtw_0eHw"
   },
   "source": [
    "### 2.2\n",
    "In this exercise, you will **implement the `predict_proba` method** of this class, which is analogous to the method you implemented earlier for the `LDAClassifier`.\n",
    "\n",
    "Run the cell below after you implement the function and make sure all the assertions pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlfKLbTy3YsP"
   },
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "softmax_clf = SoftmaxRegressionClassifier()\n",
    "\n",
    "# since we haven't trained the classifier yet, we initialize the model parameters\n",
    "# with some random values just for testing `predict_proba`\n",
    "softmax_clf.weights = np.random.randn(11, 6)\n",
    "softmax_clf.biases = np.random.randn(6)\n",
    "\n",
    "# predict the class probabilities for the training data\n",
    "probs = softmax_clf.predict_proba(X_train)\n",
    "\n",
    "assert probs is not None, (\"`predict_proba` method not implemented yet or \"\n",
    "\"not returning a valid array.\")\n",
    "assert probs.shape == (len(X_train), 6), (\"`predict_proba` output should \"\n",
    "\"have shape (num_examples, num_classes).\")\n",
    "assert np.all((probs >= 0) * (probs <= 1)), (\"`predict_proba` output should be \"\n",
    "\"probabilities.\")\n",
    "assert np.allclose(probs.sum(axis=1), 1), (\"`predict_proba` output should be a \"\n",
    "\"(proper) probability distribution.\")\n",
    "print(\"Your `predict_proba` looks good! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNW8ozRG2YKW"
   },
   "source": [
    "### 2.3\n",
    "Now you should **implement the `compute_loss_and_gradients` method**, which should compute the loss $\\ell(\\mathbf{\\theta})$ and the gradients $\\nabla_{\\mathbf{w}_y} \\ell(\\mathbf{\\theta})$ and $\\nabla_{b_y} \\ell(\\mathbf{\\theta})$ of $\\ell(\\mathbf{\\theta})$ with respect to $\\mathbf{w}_y$ and $b_y$, respectively.\n",
    "\n",
    "To save you some work, we provide you with the formulas for the gradients (if you're curious and brave enough, we encourage you to try to derive these formulas by yourself ðŸ’ª):\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\nabla_{\\mathbf{w}_y} \\ell(\\mathbf{\\theta}) = \\frac{1}{n}\\sum_{i=1}^n \\left(p(y \\mid \\mathbf{x}_i; \\mathbf{\\theta}) - 1(y_i = y)\\right)\\mathbf{x}_i,\\\\\n",
    "&\\nabla_{b_y} \\ell(\\mathbf{\\theta}) = \\frac{1}{n}\\sum_{i=1}^n \\left(p(y \\mid \\mathbf{x}_i; \\mathbf{\\theta}) - 1(y_i = y)\\right),\n",
    "\\end{align}$$\n",
    "\n",
    "where $1(\\cdot)$ denotes the indicator function (i.e., $1(\\text{condition}) = 1$ if the $\\text{condition}$ is true, and $1(\\text{condition}) = 0$ if the $\\text{condition}$ is false).\n",
    "\n",
    "**Hint:** Use the function `predict_proba`.\n",
    "\n",
    "Run the cell below after you implement the function and make sure all the assertions pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05a-SBrAz7HU"
   },
   "outputs": [],
   "source": [
    "# instantiate the classifier\n",
    "# (we need to do this again because you probably changed the code of SoftmaxRegressionClassifier\n",
    "# after you ran the previous cell)\n",
    "softmax_clf = SoftmaxRegressionClassifier()\n",
    "\n",
    "# since we haven't trained the classifier yet, we initialize the model parameters\n",
    "# with some random values just for testing `predict_proba`\n",
    "softmax_clf.weights = np.random.randn(11, 6)\n",
    "softmax_clf.biases = np.random.randn(6)\n",
    "\n",
    "loss_and_grads = softmax_clf.compute_loss_and_gradients(X_train, y_train)\n",
    "\n",
    "assert loss_and_grads is not None, (\"`compute_gradients` method not implemented yet or \"\n",
    "\"not returning a valid tuple.\")\n",
    "assert len(loss_and_grads) == 3, (\"`compute_gradients` should return a tuple with three \"\n",
    "\"elements.\")\n",
    "loss, dweights, dbiases = loss_and_grads\n",
    "assert isinstance(loss, float), (\"the loss value should be a float.\")\n",
    "assert loss >= 0, (f\"the loss value cannot be negative, got {loss:.3f}.\")\n",
    "assert isinstance(dweights, np.ndarray) and isinstance(dbiases, np.ndarray), (\n",
    "\"gradients should be NumPy arrays.\")\n",
    "assert dweights.shape == softmax_clf.weights.shape and dbiases.shape == softmax_clf.biases.shape, (\n",
    "\"`compute_gradients` output should have the correct shapes.\")\n",
    "print(\"Your `compute_gradients` looks good! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ed68lgs9ruo"
   },
   "source": [
    "### 2.4\n",
    "We are finally ready to **implement the `fit` method**, which should perform `max_iterations` iterations of **gradient descent** with the given `learning_rate` to estimate the `weights` and `biases` of the model. The method should stop earlier if the loss improvement is less than `tolerance` from one iteration to the next.\n",
    "\n",
    "The inputs of this method are again analogous to those of the `fit` method of the `LDAClassifier`. However, now we want to **return an array with the loss values** on each training iteration, so that we can plot the loss curve afterwards.\n",
    "\n",
    "Run the cell below after you have implemented the function and make sure that all the assertions pass. A plot showing the evolution of the loss during training will be generated, and you should also make sure that it looks as you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLJrKd8Q9ncf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# instantiate the classifier\n",
    "# (we need to do this again because you probably changed the code of SoftmaxRegressionClassifier\n",
    "# after you ran the previous cell)\n",
    "softmax_clf = SoftmaxRegressionClassifier()\n",
    "# train the classifier\n",
    "loss_hist = softmax_clf.fit(X_train, y_train)\n",
    "\n",
    "assert softmax_clf.weights is not None, (\"`fit` method not implemented yet or not \"\n",
    "\"updating the `weights` attribute.\")\n",
    "assert softmax_clf.biases is not None, (\"`fit` method not implemented yet or not \"\n",
    "\"updating the `biases` attribute.\")\n",
    "assert softmax_clf.weights.shape == (11, 6) and softmax_clf.biases.shape == (6,), (\"`fit` \"\n",
    "\"method should create the `weights` and `biases` attributes with the \"\n",
    "\"correct shapes.\")\n",
    "assert loss_hist is not None, (\"`fit` method not implemented yet or not \"\n",
    "\"returning a valid array.\")\n",
    "\n",
    "# plot the evolution of the loss during training\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolution of the loss during training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0K95JWSAGVy"
   },
   "source": [
    "### 2.5\n",
    "To finish the implementation of the classifier, you still need to **implement the `predict` method**, which again is analogous to the same method of the `LDAClassifier`.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass and that the values of the training and test accuracies are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVdmrqVtAj5q"
   },
   "outputs": [],
   "source": [
    "# instantiate the classifier and train it\n",
    "# (we need to do this again because you probably changed the code of SoftmaxRegressionClassifier\n",
    "# after you ran the previous cell)\n",
    "softmax_clf = SoftmaxRegressionClassifier()\n",
    "softmax_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the class labels for the training data\n",
    "y_train_pred = softmax_clf.predict(X_train)\n",
    "assert y_train_pred is not None, (\"`predict` method not implemented yet or not returning \"\n",
    "\"any predictions.\")\n",
    "assert len(y_train_pred) == len(X_train), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_train_pred == j).sum()} training examples in class {j}.\")\n",
    "print()\n",
    "\n",
    "# predict the class labels for the test data\n",
    "y_test_pred = softmax_clf.predict(X_test)\n",
    "assert y_test_pred is not None, (\"`predict` method not implemented yet or not returning \"\n",
    "\"any predictions.\")\n",
    "assert len(y_test_pred) == len(X_test), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_test_pred == j).sum()} test examples in class {j}.\")\n",
    "print()\n",
    "\n",
    "# compute the training accuracy\n",
    "train_accuracy = compute_accuracy(y_train, y_train_pred)\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "# compute the test accuracy\n",
    "test_accuracy = compute_accuracy(y_test, y_test_pred)\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMhAg1J5Bs17"
   },
   "source": [
    "### 2.6\n",
    "Based on the accuracy values, **which of the two classifiers**, `SoftmaxRegClassifier` or `LDAClassifier`, **would you choose** to solve this classification problem? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD4p1_abCIhh"
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7Z_fOwnCYv6"
   },
   "source": [
    "### 2.7\n",
    "Explain why normalizing features to have zero mean and unit variance is an important preprocessing step **for this classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbOlIL17Caqv"
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldt7X-QQCLei"
   },
   "source": [
    "## Problem 3: Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diJCtaaSChyI"
   },
   "source": [
    "In problems 1 and 2, you built two different classifiers for the in-hands problem. Now, you will build an *ensemble* using those classifiers, i.e. you will create a new classifier that uses the predictions of the individual classifiers and combines them to produce the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA0hdzfGD_MH"
   },
   "source": [
    "### 3.1\n",
    "There are several different strategies to create classifier ensembles (e.g., hard voting, soft voting, or weighted voting). **Propose and explain the ensembling strategy** that you will use to combine the two classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WRqX7KeESfL"
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyqfeSkfEVkJ"
   },
   "source": [
    "### 3.2\n",
    "Complete the code of the function `ensemble_predict`, whose skeleton is given below, by **implementing the ensembling strategy you proposed** in the previous question. This function receives:\n",
    "- `X`: An array of shape `(num_examples, num_features)` containing the input examples $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n$ for which predictions are needed.\n",
    "- `lda_clf`: An instance of `LDACLassifier`.\n",
    "- `softmax_clf`: An instance of `SoftmaxRegressionClassifier`.\n",
    "\n",
    "The function should return an array of shape `(num_examples,)` with the predicted labels for each input example.\n",
    "\n",
    "After implementing the function, run the cell below to make sure that all the assertions pass and that the training and test accuracy values are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsXEbKKdBsEK"
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(\n",
    "    X: np.ndarray,\n",
    "    lda_clf: LDAClassifier,\n",
    "    softmax_clf: SoftmaxRegressionClassifier,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the class labels for the given input data using an ensemble classifier.\n",
    "\n",
    "    Parameters:\n",
    "    X: np.ndarray of shape (num_examples, num_features)\n",
    "        The input data (features).\n",
    "    lda_clf: LDAClassifier\n",
    "        The LDAClassifier instance.\n",
    "    softmax_clf: SoftmaxRegressionClassifier\n",
    "        The SoftmaxRegressionClassifier instance.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray of shape (num_examples,)\n",
    "        The predicted class labels.\n",
    "    \"\"\"\n",
    "    # ToDo: Exercise 3.2\n",
    "    # **Replace `pass` with your code**\n",
    "    pass\n",
    "\n",
    "\n",
    "# predict the class labels for the training data\n",
    "y_train_pred = ensemble_predict(X_train, lda_clf, softmax_clf)\n",
    "assert y_train_pred is not None, (\"`ensemble_predict` method not implemented yet\"\n",
    "\" or not returning any predictions.\")\n",
    "assert len(y_train_pred) == len(X_train), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_train_pred == j).sum()} training examples in class {j}.\")\n",
    "print()\n",
    "\n",
    "# predict the class labels for the test data\n",
    "y_test_pred = ensemble_predict(X_test, lda_clf, softmax_clf)\n",
    "assert y_test_pred is not None, (\"`ensemble_predict` method not implemented yet \"\n",
    "\"or not returning any predictions.\")\n",
    "assert len(y_test_pred) == len(X_test), (\"You should have one predicted \"\n",
    "\"label for each input example.\")\n",
    "print(\"Your classifier predicted:\")\n",
    "for j in range(6):\n",
    "    print(f\"  * {(y_test_pred == j).sum()} test examples in class {j}.\")\n",
    "print()\n",
    "\n",
    "# compute the training accuracy\n",
    "train_accuracy = compute_accuracy(y_train, y_train_pred)\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "# compute the test accuracy\n",
    "test_accuracy = compute_accuracy(y_test, y_test_pred)\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA-M_lTwHdBE"
   },
   "source": [
    "### 3.3\n",
    "Based on the accuracy values, **would you prefer the ensemble classifier or one of the classifiers individually**? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE58fYIKHpli"
   },
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
